{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from patchify import patchify, unpatchify\n",
    "import seaborn as sns\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import tensorflow as tf\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting image paths for SSID dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = os.listdir('datasets/SIDD_Medium_Srgb/Data')\n",
    "\n",
    "folders = []\n",
    "GT = []\n",
    "Noisy = []\n",
    "\n",
    "for folder in dir:\n",
    "    folders.append(folder)\n",
    "\n",
    "for folder in folders:\n",
    "    files = os.listdir('datasets/SIDD_Medium_Srgb/Data/' + folder)\n",
    "    for image in files:\n",
    "        if image[5] == 'G':\n",
    "            GT.append('datasets/SIDD_Medium_Srgb/Data/' + folder + '/' + image)\n",
    "        else:\n",
    "            Noisy.append('datasets/SIDD_Medium_Srgb/Data/' + folder + '/' + image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting image paths for RENOIR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = os.listdir('datasets/Mi3_Aligned')\n",
    "\n",
    "folders = []\n",
    "GT = []\n",
    "Noisy = []\n",
    "\n",
    "for folder in dir:\n",
    "    folders.append(folder)\n",
    "\n",
    "for folder in folders:\n",
    "    added = False\n",
    "\n",
    "    if folder == '.DS_Store':\n",
    "        continue\n",
    "\n",
    "    files = os.listdir('datasets/Mi3_Aligned/' + folder)\n",
    "\n",
    "    for image in files:\n",
    "        if 'Reference.bmp' in image:\n",
    "            GT.append('datasets/Mi3_Aligned/' + folder + '/' + image)\n",
    "        \n",
    "        if 'Noisy.bmp' in image and not added:\n",
    "            Noisy.append('datasets/Mi3_Aligned/' + folder + '/' + image)\n",
    "            added = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['Ground Truth Images'] = GT\n",
    "df['Noisy Images'] = Noisy\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = []\n",
    "for i in range(len(df)):\n",
    "    img_gt = cv2.imread(df['Ground Truth Images'].iloc[i])\n",
    "    size.append(img_gt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['image size'] = size\n",
    "df['image size'] = df['image size'].astype(str)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(100, 10))\n",
    "y = list(df['image size'].value_counts())\n",
    "x = df['image size'].value_counts().index.to_list()\n",
    "plt.bar(x, y)\n",
    "plt.title(\"Images vs Size\")\n",
    "plt.xlabel(\"Size of Images\")\n",
    "plt.ylabel(\"Number of Images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying few clean-noisy image pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df.sample(3)\n",
    "fig, ax = plt.subplots(len(sample), 2, figsize=(30, 30))\n",
    "for i in range(len(sample)):\n",
    "    img = cv2.imread(sample['Ground Truth Images'].iloc[i])\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (512, 512))\n",
    "    ax[i][0].imshow(img)\n",
    "    ax[i][0].get_xaxis().set_visible(False)\n",
    "    ax[i][0].get_yaxis().set_visible(False)\n",
    "    ax[i][0].title.set_text(\"Ground Truth Images\")\n",
    "\n",
    "    img = cv2.imread(sample['Noisy Images'].iloc[i])\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (512, 512))\n",
    "    ax[i][1].imshow(img)\n",
    "    ax[i][1].get_xaxis().set_visible(False)\n",
    "    ax[i][1].get_yaxis().set_visible(False)\n",
    "    ax[i][1].title.set_text(\"Noisy Images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset into test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['Noisy Images']\n",
    "y = df['Ground Truth Images']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing the image patches as pickle files\n",
    "pickle.dump((X_train, y_train), open('train_path.pkl', 'wb'))\n",
    "pickle.dump((X_test, y_test), open('test_path.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = pickle.load(open('train_path.pkl', \"rb\"))\n",
    "X_test, y_test = pickle.load(open('test_path.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patches(img, patch_size):\n",
    "    patches = patchify(img, (patch_size, patch_size, 3), step=patch_size)\n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_patches = []\n",
    "y_train_patches = []\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    path = X_train.iloc[i]\n",
    "    img_nsy = cv2.imread(path)\n",
    "    img_nsy = cv2.cvtColor(img_nsy, cv2.COLOR_BGR2RGB)\n",
    "    img_nsy = cv2.resize(img_nsy, (1024, 1024))\n",
    "    patches_nsy = patches(img_nsy, 256)\n",
    "\n",
    "    path = X_train.iloc[i]\n",
    "    img_gt = cv2.imread(path)\n",
    "    img_gt = cv2.cvtColor(img_gt, cv2.COLOR_BGR2RGB)\n",
    "    img_gt = cv2.resize(img_gt, (1024, 1024))\n",
    "    patches_gt = patches(img_gt, 256)\n",
    "\n",
    "    rows = patches_nsy.shape[0]\n",
    "    cols = patches_nsy.shape[1]\n",
    "\n",
    "    for j in range(rows):\n",
    "        for k in range(cols):\n",
    "            X_train_patches.append(patches_nsy[j][k][0])\n",
    "            y_train_patches.append(patches_gt[j][k][0])\n",
    "\n",
    "X_train = np.array(X_train_patches)\n",
    "y_train = np.array(y_train_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_patches = []\n",
    "y_test_patches = []\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    path = X_test.iloc[i]\n",
    "    img_nsy = cv2.imread(path)\n",
    "    img_nsy = cv2.cvtColor(img_nsy, cv2.COLOR_BGR2RGB)\n",
    "    img_nsy = cv2.resize(img_nsy,(1024,1024))  #resizing the X_test images\n",
    "    patches_nsy = patches(img_nsy,256)\n",
    "\n",
    "    path = y_test.iloc[i]\n",
    "    img_gt = cv2.imread(path)\n",
    "    img_gt = cv2.cvtColor(img_gt, cv2.COLOR_BGR2RGB)\n",
    "    img_gt = cv2.resize(img_gt,(1024,1024))  #resizing the y_test images\n",
    "    patches_gt = patches(img_gt,256)\n",
    "\n",
    "    rows = patches_nsy.shape[0]\n",
    "    cols = patches_nsy.shape[1]\n",
    "\n",
    "    for j in range(rows):\n",
    "        for k in range(cols):\n",
    "            X_test_patches.append(patches_nsy[j][k][0])\n",
    "            y_test_patches.append(patches_gt[j][k][0])\n",
    "\n",
    "X_test = np.array(X_test_patches)\n",
    "y_test = np.array(y_test_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,5,figsize=(20,10))\n",
    "r = random.sample(range(0, 2048), 5)\n",
    "\n",
    "fig.suptitle('Train Image Patches',fontweight =\"bold\")\n",
    "for i in range(5):\n",
    "    axs[0][i].imshow(y_train[r[i]])\n",
    "    axs[0][i].set_title('Ground Truth Image Patches')\n",
    "    axs[1][i].imshow(X_train[r[i]])\n",
    "    axs[1][i].set_title('Noisy Image Patches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,5,figsize=(20,10))\n",
    "r = random.sample(range(0, 512), 5)\n",
    "\n",
    "fig.suptitle('Test Image Patches',fontweight =\"bold\")\n",
    "for i in range(5):\n",
    "    axs[0][i].imshow(y_test[r[i]])\n",
    "    axs[0][i].set_title('Ground Truth Image Patches')\n",
    "    axs[1][i].imshow(X_test[r[i]])\n",
    "    axs[1][i].set_title('Noisy Image Patches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total number of image patches on train data : \", len(X_train))\n",
    "print(\"Total number of image patches on test data : \", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump((X_train,y_train),open('train_data.pkl','wb'))\n",
    "pickle.dump((X_test,y_test),open('test_data.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoising few image patches using NLM filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axis = plt.subplots(3, 4, figsize=(30, 15))\n",
    "r = random.sample(range(0, rows), 4)\n",
    "c = random.sample(range(0, cols), 4)\n",
    "PSNR_nsy = []\n",
    "PSNR_de_nsy = []\n",
    "\n",
    "for i in range(4):\n",
    "    img1 = patches_nsy[r[i]][c[i]][0]\n",
    "    img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "    img2 = patches_gt[r[i]][c[i]][0]\n",
    "    img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    axis[0][i].imshow(img1)\n",
    "    axis[0][i].set_title('Noisy Image Patches')\n",
    "    axis[1][i].imshow(img2)\n",
    "    axis[1][i].set_title('GT Images Patches')\n",
    "\n",
    "    # Denoising\n",
    "    dst = cv2.fastNlMeansDenoisingColored(img1, None, 50, 10, 21, 7)\n",
    "    axis[2][i].imshow(dst)\n",
    "    axis[2][i].set_title('Denoised Image Patches')\n",
    "\n",
    "    PSNR_nsy.append(psnr(img1, img2))\n",
    "    PSNR_de_nsy.append(psnr(img1, dst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "improvement = [x1 - x2 for(x1, x2) in zip(PSNR_de_nsy, PSNR_nsy)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "x = PrettyTable()\n",
    "x.add_column(\"PSNR before denoising\", PSNR_nsy)\n",
    "x.add_column(\"PSNR after denoising\", PSNR_de_nsy)\n",
    "x.add_column(\"PSNR Improvement\", improvement)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Dataset for modeling using custom data generators in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom function to get denoised image prediction for noisy images\n",
    "def prediction(img,model):\n",
    "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "  img = cv2.resize(img,(1024,1024))\n",
    "  img = img.astype(\"float32\") / 255.0\n",
    "\n",
    "  img_patches = patches(img,256)\n",
    "\n",
    "  nsy=[]\n",
    "  for i in range(4):\n",
    "    for j in range(4):\n",
    "      nsy.append(img_patches[i][j][0])\n",
    "  nsy = np.array(nsy)\n",
    "\n",
    "  pred_img = model.predict(nsy)\n",
    "  pred_img = np.reshape(pred_img,(4,4,1,256,256,3))\n",
    "  pred_img = unpatchify(pred_img, img.shape)\n",
    "  return pred_img\n",
    "\n",
    "\n",
    "#Custom function to get denoised image prediction for noisy images on quantized models using tflite\n",
    "def prediction_tflite(img,model):\n",
    "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "  img = cv2.resize(img,(1024,1024))\n",
    "  img = img.astype(\"float32\") / 255.0\n",
    "\n",
    "  img_patches = patches(img,256)\n",
    "\n",
    "  nsy=[]\n",
    "  for i in range(4):\n",
    "    for j in range(4):\n",
    "      nsy.append(img_patches[i][j][0])\n",
    "  nsy = np.array(nsy)\n",
    "  pred=[]\n",
    "  for patch in nsy:\n",
    "    model.set_tensor(input_details[0]['index'], tf.expand_dims(patch,axis=0))\n",
    "    model.invoke()\n",
    "    tflite_model_predictions = model.get_tensor(output_details[0]['index'])\n",
    "    pred.append(tflite_model_predictions)\n",
    "\n",
    "  pred_img = np.reshape(pred,(4,4,1,256,256,3))\n",
    "  pred_img = unpatchify(pred_img, img.shape)\n",
    "  return pred_img\n",
    "\n",
    "#Custom function to plot/visualize noisy, ground truth and predicted images\n",
    "def visualize(sample,model):\n",
    "  fig,ax = plt.subplots(len(sample),3,figsize=(30,30))\n",
    "  for i in range(len(sample)):\n",
    "    path = sample['Ground Truth Images'].iloc[i]\n",
    "    test_img_gt = cv2.imread(path)\n",
    "    test_img_gt = cv2.cvtColor(test_img_gt, cv2.COLOR_BGR2RGB)\n",
    "    test_img_gt = cv2.resize(test_img_gt,(512,512))\n",
    "    test_img_gt = test_img_gt.astype(\"float32\") / 255.0\n",
    "  \n",
    "    path = sample['Noisy Images'].iloc[i]\n",
    "    test_img_nsy = cv2.imread(path)\n",
    "    pred_img = prediction(test_img_nsy,model)\n",
    "    pred_img = cv2.resize(pred_img,(512,512))\n",
    "\n",
    "    test_img_nsy = cv2.cvtColor(test_img_nsy, cv2.COLOR_BGR2RGB)\n",
    "    test_img_nsy = cv2.resize(test_img_nsy,(512,512))\n",
    "    test_img_nsy = test_img_nsy.astype(\"float32\") / 255.0\n",
    "    \n",
    "    ax[i][0].imshow(test_img_nsy)\n",
    "    ax[i][0].get_xaxis().set_visible(False)\n",
    "    ax[i][0].get_yaxis().set_visible(False)\n",
    "    ax[i][0].title.set_text(\"Noisy Image\")\n",
    "\n",
    "    ax[i][1].imshow(test_img_gt)\n",
    "    ax[i][1].get_xaxis().set_visible(False)\n",
    "    ax[i][1].get_yaxis().set_visible(False)\n",
    "    ax[i][1].title.set_text(\"Ground Truth Image\")\n",
    "\n",
    "    ax[i][2].imshow(pred_img)\n",
    "    ax[i][2].get_xaxis().set_visible(False)\n",
    "    ax[i][2].get_yaxis().set_visible(False)\n",
    "    ax[i][2].title.set_text(\"Predicted Image\")\n",
    "\n",
    "#Custom function that computes the psnr and ssim values for images\n",
    "def psnr_and_ssim(X_test,y_test,model,model_type='Normal'):\n",
    "  psnr_nsy = 0.0\n",
    "  psnr_de_nsy = 0.0\n",
    "  ssim_nsy = 0.0\n",
    "  ssim_de_nsy = 0.0\n",
    "  for i in range(len(X_test)):\n",
    "    #getting the noisy images\n",
    "    path = X_test.iloc[i]\n",
    "    nsy = cv2.imread(path)  \n",
    "\n",
    "    #getting the predicted images\n",
    "    if model_type == 'Quantized': \n",
    "      pred = prediction_tflite(nsy,model)\n",
    "    else:\n",
    "      pred = prediction(nsy,model)\n",
    "\n",
    "    #getting the ground truth images\n",
    "    path = y_test.iloc[i]\n",
    "    gt = cv2.imread(path)         \n",
    "    gt = cv2.cvtColor(gt, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    #Resizing the images\n",
    "    gt = cv2.resize(gt,(1024,1024))\n",
    "    nsy = cv2.resize(nsy,(1024,1024))\n",
    "\n",
    "    #Normalizing the images\n",
    "    gt = gt.astype(\"float32\") / 255.0\n",
    "    nsy = nsy.astype(\"float32\") / 255.0\n",
    "\n",
    "    #Computing psnr and ssim for test images\n",
    "    psnr_nsy += psnr(gt,nsy)\n",
    "    psnr_de_nsy += psnr(gt,pred)\n",
    "    ssim_nsy += ssim(gt,nsy,multichannel=True,data_range=nsy.max() - nsy.min())\n",
    "    ssim_de_nsy += ssim(gt,pred,multichannel=True,data_range=pred.max() - pred.min())\n",
    "\n",
    "  psnr_nsy = psnr_nsy/len(X_test)\n",
    "  psnr_de_nsy = psnr_de_nsy/len(X_test)\n",
    "  ssim_nsy = ssim_nsy/len(X_test)\n",
    "  ssim_de_nsy = ssim_de_nsy/len(X_test)\n",
    "  return psnr_nsy, psnr_de_nsy,ssim_nsy,ssim_de_nsy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_patches, y_train_patches = pickle.load(open('train_data.pkl', 'rb'))\n",
    "X_test_patches,y_test_patches = pickle.load(open('test_data.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4096, 256, 256, 3)\n",
      "(4096, 256, 256, 3)\n",
      "(1024, 256, 256, 3)\n",
      "(1024, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_patches.shape)\n",
    "print(y_train_patches.shape)\n",
    "print(X_test_patches.shape)\n",
    "print(y_test_patches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the image pixels\n",
    "X_train_patches = X_train_patches.astype('float32') / 255.0\n",
    "y_train_patches = y_train_patches.astype('float32') / 255.0\n",
    "X_test_patches = X_test_patches.astype('float32') / 255.0\n",
    "y_test_patches = y_test_patches.astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataloader(tf.keras.utils.Sequence):\n",
    "    def __init__(self, X, y, batch_size=1, shuffle=False):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indexes = np.arange(len(X))\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        batch_x = self.X[i * self.batch_size : (i + 1) * self.batch_size]\n",
    "        batch_y= self.y[i * self.batch_size : (i + 1) * self.batch_size]\n",
    "        return tuple((batch_x, batch_y))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indexes)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            self.indexes = np.random.permutation(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dataloader = Dataloader(X_train_patches,y_train_patches, batch_size, shuffle=True)\n",
    "test_dataloader = Dataloader(X_test_patches,y_test_patches,batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 256, 256, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model: Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler,ReduceLROnPlateau\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, BatchNormalization, Activation, Flatten, Dense, Input, MaxPooling2D, Add, Reshape, concatenate, AveragePooling2D, Multiply, GlobalAveragePooling2D, UpSampling2D, MaxPool2D,Softmax\n",
    "from tensorflow.keras.activations import softmax\n",
    "from tensorflow.keras import initializers, regularizers\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "input = Input(shape=(256, 256, 3))\n",
    "\n",
    "# Encoder\n",
    "x = Conv2D(32, (3, 3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(input)\n",
    "x = MaxPooling2D((2, 2), padding=\"same\")(x)\n",
    "x = Conv2D(64, (3, 3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(x)\n",
    "x = MaxPooling2D((2, 2), padding=\"same\")(x)\n",
    "x = Conv2D(128, (3, 3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(x)\n",
    "x = MaxPooling2D((2, 2), padding=\"same\")(x)\n",
    "\n",
    "# Decoder\n",
    "x = Conv2DTranspose(128, (3, 3), strides=2, activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(x)\n",
    "x = Conv2DTranspose(64, (3, 3), strides=2, activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(x)\n",
    "x = Conv2DTranspose(32, (3, 3), strides=2, activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(x)\n",
    "x = Conv2D(3, (3, 3), activation=\"sigmoid\", kernel_initializer=\"he_normal\", padding=\"same\")(x)\n",
    "\n",
    "# Autoencoder\n",
    "autoencoder = Model(input, x)\n",
    "autoencoder.compile(optimizer=tf.keras.optimizers.Adam(1e-03), loss=tf.keras.losses.MeanSquaredError())\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPU Avaliable:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPU Avaliable: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = 'logs/model_1'\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True, write_grads=True)\n",
    "reducelr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, verbose=1, patience=2)\n",
    "callback = [tensorboard, reducelr]\n",
    "\n",
    "autoencoder.fit(train_dataloader, shuffle=True, epochs=20, validation_data=test_dataloader, callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir \"logs/model_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.save('models/autoencoder', save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = tf.keras.models.load_model('models/autoencoder.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.DataFrame({'Ground Truth Images' : ['datasets/SIDD_Small_sRGB_Only/Data/0001_001_S6_00100_00060_3200_L/GT_SRGB_010.PNG','datasets/SIDD_Small_sRGB_Only/Data/0002_001_S6_00100_00020_3200_N/GT_SRGB_010.PNG','datasets/SIDD_Small_sRGB_Only/Data/0003_001_S6_00100_00060_3200_H/GT_SRGB_010.PNG'], 'Noisy Images' : ['datasets/SIDD_Small_sRGB_Only/Data/0001_001_S6_00100_00060_3200_L/NOISY_SRGB_010.PNG','datasets/SIDD_Small_sRGB_Only/Data/0002_001_S6_00100_00020_3200_N/NOISY_SRGB_010.PNG','datasets/SIDD_Small_sRGB_Only/Data/0003_001_S6_00100_00060_3200_H/NOISY_SRGB_010.PNG']})\n",
    "#sample = pd.DataFrame({'Ground Truth Images' : ['datasets/SIDD_Medium_Srgb/Data/0073_003_IP_00200_01000_5500_L/0073_GT_SRGB_010.PNG'], 'Noisy Images' : ['datasets/SIDD_Medium_Srgb/Data/0073_003_IP_00200_01000_5500_L/0073_NOISY_SRGB_010.PNG']})\n",
    "#sample = pd.DataFrame({'Ground Truth Images' : ['datasets/GT2.png'], 'Noisy Images' : ['datasets/N2.png']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(sample, autoencoder)\n",
    "\n",
    "path = sample['Ground Truth Images'].iloc[0]\n",
    "test_img_gt = cv2.imread(path)\n",
    "test_img_gt = cv2.cvtColor(test_img_gt, cv2.COLOR_BGR2RGB)\n",
    "test_img_gt = cv2.resize(test_img_gt, (512, 512))\n",
    "test_img_gt = test_img_gt.astype(\"float32\") / 255.0\n",
    "\n",
    "path = sample['Noisy Images'].iloc[0]\n",
    "test_img_nsy = cv2.imread(path)\n",
    "\n",
    "pred = prediction(test_img_nsy, autoencoder)\n",
    "pred = cv2.resize(pred, (512, 512))\n",
    "\n",
    "test_img_nsy= cv2.cvtColor(test_img_nsy, cv2.COLOR_BGR2RGB)\n",
    "test_img_nsy = cv2.resize(test_img_nsy, (512, 512))\n",
    "test_img_nsy = test_img_nsy.astype(\"float32\") / 255.0\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(30,30))\n",
    "ax[0].imshow(test_img_nsy)\n",
    "ax[0].title.set_text('Noisy')\n",
    "ax[1].imshow(test_img_gt)\n",
    "ax[1].title.set_text('GT')\n",
    "ax[2].imshow(pred)\n",
    "ax[2].title.set_text('Pred')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr_nsy, psnr_de_nsy, ssim_nsy, ssim_de_nsy = psnr_and_ssim(X_test,y_test,autoencoder)\n",
    "print('PSNR before denoising :', psnr_nsy)\n",
    "print('PSNR after denoising :', psnr_de_nsy)\n",
    "print('SSIM before denoising :', ssim_nsy)\n",
    "print('SSIM after denoising :', ssim_de_nsy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RIDNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EAM(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.conv1 = Conv2D(64, (3, 3), dilation_rate=1, padding='same', activation='relu')\n",
    "        self.conv2 = Conv2D(64, (3, 3), dilation_rate=2, padding='same', activation='relu')\n",
    "\n",
    "        self.conv3 = Conv2D(64, (3, 3), dilation_rate=3, padding='same', activation='relu')\n",
    "        self.conv4 = Conv2D(64, (3, 3), dilation_rate=4, padding='same', activation='relu')\n",
    "\n",
    "        self.conv5 = Conv2D(64, (3, 3), padding='same', activation='relu')\n",
    "\n",
    "        self.conv6 = Conv2D(64, (3, 3), padding='same', activation='relu')\n",
    "        self.conv7 = Conv2D(64, (3, 3), padding='same')\n",
    "\n",
    "        self.conv8 = Conv2D(64, (3, 3), padding='same', activation='relu')\n",
    "        self.conv9 = Conv2D(64, (3, 3), padding='same', activation='relu')\n",
    "        self.conv10 = Conv2D(64, (1, 1), padding='same')\n",
    "\n",
    "        self.gap = GlobalAveragePooling2D()\n",
    "\n",
    "        self.conv11 = Conv2D(64, (3, 3), padding='same', activation='relu')\n",
    "        self.conv12 = Conv2D(64, (3, 3), padding='same', activation='sigmoid')\n",
    "    \n",
    "    def call(self, input):\n",
    "        conv1 = self.conv1(input)\n",
    "        conv1 = self.conv1(conv1)\n",
    "\n",
    "        conv2 = self.conv3(input)\n",
    "        conv2 = self.conv4(conv2)\n",
    "\n",
    "        concat = concatenate([conv1, conv2])\n",
    "        conv3 = self.conv5(concat)\n",
    "        add1 = Add()([input, conv3])\n",
    "\n",
    "        conv4 = self.conv6(add1)\n",
    "        conv4 = self.conv7(conv4)\n",
    "        add2 = Add()([conv4, add1])\n",
    "        add2 = Activation('relu')(add2)\n",
    "\n",
    "        conv5 = self.conv8(add2)\n",
    "        conv5 = self.conv9(conv5)\n",
    "        conv5 = self.conv10(conv5)\n",
    "        add3 = Add()([add2, conv5])\n",
    "        add3 = Activation('relu')(add3)\n",
    "\n",
    "        gap = self.gap(add3)\n",
    "        gap = Reshape((1, 1, 64))(gap)\n",
    "        conv6 = self.conv11(gap)\n",
    "        conv6 = self.conv11(conv6)\n",
    "\n",
    "        mul = Multiply()([conv6, add3])\n",
    "        out = Add()([input, mul])\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "input = Input(shape=(256, 256, 3))\n",
    "\n",
    "conv1 = Conv2D(64, (3, 3), padding='same')(input)\n",
    "eam1 = EAM()(conv1)\n",
    "eam2 = EAM()(eam1)\n",
    "eam3 = EAM()(eam2)\n",
    "eam4 = EAM()(eam3)\n",
    "conv2 = Conv2D(3, (3, 3), padding='same')(eam4)\n",
    "out = Add()([conv2, input])\n",
    "\n",
    "RIDNet = Model(input, out)\n",
    "RIDNet.compile(optimizer=tf.keras.optimizers.Adam(1e-03), loss=tf.keras.losses.MeanSquaredError())\n",
    "RIDNet.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=8\n",
    "train_dataloader = Dataloader(X_train_patches,y_train_patches, batch_size, shuffle=True)\n",
    "test_dataloader = Dataloader(X_test_patches,y_test_patches,batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir=\"logs/model_4\"\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=1, write_graph=True,write_grads=True)\n",
    "reducelr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,verbose=1,patience=4,min_delta=0.00001)\n",
    "callback = [tensorboard,reducelr]\n",
    "RIDNet.fit(train_dataloader,shuffle=True,epochs=20,validation_data= test_dataloader, callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Filepath looks like a hdf5 file but h5py is not available. filepath=models/RIDNetGH.h5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Federico\\Documents\\Sviluppo\\Git\\PathTracing\\CNN-Denoiser\\denoiser.ipynb Cella 58\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Federico/Documents/Sviluppo/Git/PathTracing/CNN-Denoiser/denoiser.ipynb#Y111sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m RIDNet \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mmodels\u001b[39m.\u001b[39;49mload_model(\u001b[39m'\u001b[39;49m\u001b[39mmodels/RIDNetGH.h5\u001b[39;49m\u001b[39m'\u001b[39;49m, custom_objects\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mEAM\u001b[39;49m\u001b[39m'\u001b[39;49m: EAM})\n",
      "File \u001b[1;32mc:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\keras\\saving\\save.py:236\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    235\u001b[0m         \u001b[39mif\u001b[39;00m h5py \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 236\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[0;32m    237\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mFilepath looks like a hdf5 file but h5py is \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    238\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mnot available.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    239\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m filepath=\u001b[39m\u001b[39m{\u001b[39;00mfilepath_str\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    240\u001b[0m             )\n\u001b[0;32m    241\u001b[0m         \u001b[39mreturn\u001b[39;00m hdf5_format\u001b[39m.\u001b[39mload_model_from_hdf5(\n\u001b[0;32m    242\u001b[0m             tf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mgfile\u001b[39m.\u001b[39mGFile(filepath_str, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m    243\u001b[0m             custom_objects,\n\u001b[0;32m    244\u001b[0m             \u001b[39mcompile\u001b[39m,\n\u001b[0;32m    245\u001b[0m         )\n\u001b[0;32m    246\u001b[0m \u001b[39melif\u001b[39;00m h5py \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(filepath, h5py\u001b[39m.\u001b[39mFile):\n",
      "\u001b[1;31mImportError\u001b[0m: Filepath looks like a hdf5 file but h5py is not available. filepath=models/RIDNetGH.h5"
     ]
    }
   ],
   "source": [
    "RIDNet = tf.keras.models.load_model('models/RIDNetGH.h5', custom_objects={'EAM': EAM})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(sample, RIDNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RIDNet.save('models/RIDNet.tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBDNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 256, 56, 3)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 256, 56, 32)  896         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 256, 56, 32)  9248        ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 256, 56, 32)  9248        ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 256, 56, 32)  9248        ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 256, 56, 3)   867         ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 256, 56, 6)   0           ['conv2d_4[0][0]',               \n",
      "                                                                  'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 256, 56, 64)  3520        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 256, 56, 64)  36928       ['conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " average_pooling2d (AveragePool  (None, 128, 28, 64)  0          ['conv2d_6[0][0]']               \n",
      " ing2D)                                                                                           \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 128, 28, 128  73856       ['average_pooling2d[0][0]']      \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 128, 28, 128  147584      ['conv2d_7[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 128, 28, 128  147584      ['conv2d_8[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " average_pooling2d_1 (AveragePo  (None, 64, 14, 128)  0          ['conv2d_9[0][0]']               \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 64, 14, 256)  295168      ['average_pooling2d_1[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 64, 14, 256)  590080      ['conv2d_10[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 64, 14, 256)  590080      ['conv2d_11[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 64, 14, 256)  590080      ['conv2d_12[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 64, 14, 256)  590080      ['conv2d_13[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 64, 14, 256)  590080      ['conv2d_14[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_transpose (Conv2DTransp  (None, 128, 28, 128  295040     ['conv2d_15[0][0]']              \n",
      " ose)                           )                                                                 \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 128, 28, 128  0           ['conv2d_transpose[0][0]',       \n",
      "                                )                                 'conv2d_9[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 128, 28, 128  147584      ['add[0][0]']                    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 128, 28, 128  147584      ['conv2d_16[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 128, 28, 128  147584      ['conv2d_17[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_transpose_1 (Conv2DTran  (None, 256, 56, 64)  73792      ['conv2d_18[0][0]']              \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 256, 56, 64)  0           ['conv2d_transpose_1[0][0]',     \n",
      "                                                                  'conv2d_6[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 256, 56, 64)  36928       ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 256, 56, 64)  36928       ['conv2d_19[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 256, 56, 3)   195         ['conv2d_20[0][0]']              \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 256, 56, 3)   0           ['conv2d_21[0][0]',              \n",
      "                                                                  'input_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,570,182\n",
      "Trainable params: 4,570,182\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "input = Input(shape=(256, 56, 3))\n",
    "\n",
    "# Noise estimation subnetwork\n",
    "x = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(input)\n",
    "x = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(x)\n",
    "x = Conv2D(3, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(x)\n",
    "\n",
    "# Non Blind denoising subnetwork\n",
    "x = concatenate([x, input])\n",
    "conv1 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(x)\n",
    "conv2 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv1)\n",
    "\n",
    "pool1 = AveragePooling2D(pool_size=(2, 2), padding='same')(conv2)\n",
    "conv3 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(pool1)\n",
    "conv4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv3)\n",
    "conv5 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv4)\n",
    "\n",
    "pool2 = AveragePooling2D(pool_size=(2, 2), padding='same')(conv5)\n",
    "conv6 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(pool2)\n",
    "conv7 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv6)\n",
    "conv8 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv7)\n",
    "conv9 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv8)\n",
    "conv10 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv9)\n",
    "conv11 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv10)\n",
    "\n",
    "upsample1 = Conv2DTranspose(128, (3, 3), strides=2, activation=\"relu\", kernel_initializer='he_normal',padding=\"same\")(conv11)\n",
    "add1 = Add()([upsample1,conv5])\n",
    "conv12 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(add1)\n",
    "conv13 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv12)\n",
    "conv14 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv13)\n",
    "\n",
    "upsample2 = Conv2DTranspose(64, (3, 3), strides=2, activation=\"relu\", kernel_initializer='he_normal',padding=\"same\")(conv14)\n",
    "add1 = Add()([upsample2, conv2])\n",
    "conv15 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(add1)\n",
    "conv16 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(conv15)\n",
    "\n",
    "out = Conv2D(3, (1, 1), kernel_initializer='he_normal', padding='same')(conv16)\n",
    "out = Add()([out, input])\n",
    "\n",
    "CBDNet = Model(input, out)\n",
    "CBDNet.compile(optimizer=tf.keras.optimizers.Adam(1e-03), loss=tf.keras.losses.MeanSquaredError())\n",
    "CBDNet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`write_grads` will be ignored in TensorFlow 2.0 for the `TensorBoard` Callback.\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'model/add/add' defined at (most recent call last):\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n      app.start()\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 737, in start\n      self.io_loop.start()\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 524, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 513, in process_one\n      await dispatch(*args)\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 418, in dispatch_shell\n      await result\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 758, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 426, in do_execute\n      res = shell.run_cell(\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3024, in run_cell\n      result = self._run_cell(\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3079, in _run_cell\n      result = runner(coro)\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3284, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3466, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Federico\\AppData\\Local\\Temp\\ipykernel_34900\\2642934082.py\", line 6, in <module>\n      CBDNet.fit(train_dataloader,shuffle=True,epochs=30,validation_data= test_dataloader,callbacks=callback)\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n      y_pred = self(x, training=True)\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\keras\\layers\\merging\\base_merge.py\", line 196, in call\n      return self._merge_function(inputs)\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\keras\\layers\\merging\\add.py\", line 57, in _merge_function\n      output += inputs[i]\nNode: 'model/add/add'\nrequired broadcastable shapes\n\t [[{{node model/add/add}}]] [Op:__inference_train_function_2857]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Federico\\Documents\\Sviluppo\\Git\\PathTracing\\CNN-Denoiser\\denoiser.ipynb Cella 62\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Federico/Documents/Sviluppo/Git/PathTracing/CNN-Denoiser/denoiser.ipynb#Y115sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m reducelr \u001b[39m=\u001b[39m ReduceLROnPlateau(monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m, factor\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m,verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,patience\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m,min_delta\u001b[39m=\u001b[39m\u001b[39m0.00001\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Federico/Documents/Sviluppo/Git/PathTracing/CNN-Denoiser/denoiser.ipynb#Y115sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m callback \u001b[39m=\u001b[39m [tensorboard,reducelr]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Federico/Documents/Sviluppo/Git/PathTracing/CNN-Denoiser/denoiser.ipynb#Y115sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m CBDNet\u001b[39m.\u001b[39;49mfit(train_dataloader,shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,epochs\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m,validation_data\u001b[39m=\u001b[39;49m test_dataloader,callbacks\u001b[39m=\u001b[39;49mcallback)\n",
      "File \u001b[1;32mc:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'model/add/add' defined at (most recent call last):\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n      app.start()\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 737, in start\n      self.io_loop.start()\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 524, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 513, in process_one\n      await dispatch(*args)\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 418, in dispatch_shell\n      await result\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 758, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 426, in do_execute\n      res = shell.run_cell(\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3024, in run_cell\n      result = self._run_cell(\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3079, in _run_cell\n      result = runner(coro)\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3284, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3466, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Federico\\AppData\\Local\\Temp\\ipykernel_34900\\2642934082.py\", line 6, in <module>\n      CBDNet.fit(train_dataloader,shuffle=True,epochs=30,validation_data= test_dataloader,callbacks=callback)\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n      y_pred = self(x, training=True)\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\keras\\layers\\merging\\base_merge.py\", line 196, in call\n      return self._merge_function(inputs)\n    File \"c:\\Software\\Programs\\Conda\\envs\\tf\\lib\\site-packages\\keras\\layers\\merging\\add.py\", line 57, in _merge_function\n      output += inputs[i]\nNode: 'model/add/add'\nrequired broadcastable shapes\n\t [[{{node model/add/add}}]] [Op:__inference_train_function_2857]"
     ]
    }
   ],
   "source": [
    "log_dir=\"/logs/cbdnet\"\n",
    "\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=1, write_graph=True,write_grads=True)\n",
    "reducelr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,verbose=1,patience=4,min_delta=0.00001)\n",
    "callback = [tensorboard,reducelr]\n",
    "CBDNet.fit(train_dataloader,shuffle=True,epochs=30,validation_data= test_dataloader,callbacks=callback)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
